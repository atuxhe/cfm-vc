{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import logging\n",
    "\n",
    "from inference import infer_tool\n",
    "from inference.infer_tool import Svc\n",
    "import torch\n",
    "\n",
    "# play audio using IPython.display.Audio\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "import utils\n",
    "import random\n",
    "import soundfile\n",
    "\n",
    "\n",
    "logging.getLogger(\"numba\").setLevel(logging.WARNING)\n",
    "chunks_dict = infer_tool.read_temp(\"inference/chunks_temp.json\")\n",
    "\n",
    "\n",
    "feature_retrieval = True\n",
    "\n",
    "if feature_retrieval:\n",
    "    cluster_model_path = \"\"\n",
    "else:\n",
    "    cluster_model_path = \"logs/22k/kmeans_10000.pt\"\n",
    "\n",
    "\n",
    "sovits_model_path = \"/home/alexander/Projekte/so-vits-svc/logs/22k_cfm/G_ema_192000.pth\"\n",
    "sovits_config_path = \"/home/alexander/Projekte/so-vits-svc/logs/22k_cfm/config.json\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "svc_model = Svc(\n",
    "    net_g_path=sovits_model_path,\n",
    "    config_path=sovits_config_path,\n",
    "    cluster_model_path=cluster_model_path,\n",
    "    speaker_encoder_path=\"/home/alexander/Projekte/SpeakerEncoder/tb_logs/lightning_logs/version_15/checkpoints/checkpoint.ckpt\",\n",
    "    feature_retrieval=feature_retrieval,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "\n",
    "hps = utils.get_hparams_from_file(sovits_config_path, True)\n",
    "speaker_dict = {v: k for k, v in hps.spk.items()}\n",
    "\n",
    "speaker_files = {}\n",
    "\n",
    "with open(\n",
    "    \"/home/alexander/Projekte/so-vits-svc/filelists/gametts_train.txt\", \"r\"\n",
    ") as rf:\n",
    "    for line in rf:\n",
    "        wav_path, spk = line.split(\"|\")\n",
    "        spk = spk.strip()\n",
    "        speaker_name = speaker_dict[int(spk)]\n",
    "\n",
    "        if speaker_name not in speaker_files:\n",
    "            speaker_files[speaker_name] = []\n",
    "\n",
    "        speaker_files[speaker_name].append(wav_path)\n",
    "\n",
    "\n",
    "def inference(\n",
    "    raw_audio_path,\n",
    "    target_audio_path,\n",
    "    target_speaker=None,\n",
    "    cluster_infer_ratio=0,\n",
    "    n_timesteps=2,\n",
    "    guidance_scale=0.0,\n",
    "    temperature=1.0,\n",
    "    f0_predictor=\"crepe\",\n",
    "    solver=\"euler\",\n",
    "):\n",
    "    trans = 0\n",
    "    f0_filter_threshold = 0.05\n",
    "\n",
    "    infer_tool.mkdir([\"raw\", \"results\"])\n",
    "\n",
    "    infer_tool.format_wav(raw_audio_path)\n",
    "    for target_wav in target_audio_path:\n",
    "        infer_tool.format_wav(target_wav)\n",
    "\n",
    "    kwarg = {\n",
    "        \"raw_audio_path\": raw_audio_path,\n",
    "        \"raw_target_audio_path\": target_audio_path,\n",
    "        \"target_speaker\": target_speaker,\n",
    "        \"tran\": trans,\n",
    "        \"cluster_infer_ratio\": cluster_infer_ratio,\n",
    "        \"n_timesteps\": n_timesteps,\n",
    "        \"f0_predictor\": f0_predictor,\n",
    "        \"cr_threshold\": f0_filter_threshold,\n",
    "        \"temperature\": temperature,\n",
    "        \"guidance_scale\": guidance_scale,\n",
    "        \"solver\": solver,\n",
    "    }\n",
    "    mel = svc_model.slice_inference(**kwarg)\n",
    "\n",
    "\n",
    "    return mel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vdecoder.IstftGenerator.model import Generator\n",
    "from vdecoder.utils import AttrDict\n",
    "\n",
    "import json\n",
    "\n",
    "\n",
    "model_file = \"/home/alexander/Projekte/iSTFT-Avocodo-pytorch/cp_resnet/g_00400000\"\n",
    "config_file = \"/home/alexander/Projekte/iSTFT-Avocodo-pytorch/cp_resnet/config.json\"\n",
    "with open(config_file) as f:\n",
    "    data = f.read()\n",
    "json_config = json.loads(data)\n",
    "h = AttrDict(json_config)\n",
    "\n",
    "generator = Generator(input_channels=h.num_mels, num_layers=8).to(device)\n",
    "\n",
    "checkpoint_dict = torch.load(model_file, map_location=device)\n",
    "generator.load_state_dict(checkpoint_dict[\"generator\"])\n",
    "\n",
    "generator.eval()\n",
    "generator.remove_weight_norm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "\n",
    "# /home/alexander/Projekte/vits-emotts/samples/Test.wav\n",
    "source_audio = \"/home/alexander/Projekte/so-vits-svc/samples/4.wav\"\n",
    "target_speaker = \"Inquisitor\"\n",
    "target_audios = speaker_files[target_speaker]\n",
    "target_audios = random.sample(target_audios, len(target_audios[:10]))\n",
    "\n",
    "# target_audios = glob(\"/home/alexander/Projekte/so-vits-svc/samples/Suchender/*.wav\", recursive=True)\n",
    "# target_audios = random.sample(target_audios, len(target_audios[:10]))\n",
    "\n",
    "# target_audios = [\"/home/alexander/Projekte/so-vits-svc/samples/2.wav\", \"/home/alexander/Projekte/so-vits-svc/samples/3.wav\", \"/home/alexander/Projekte/so-vits-svc/samples/4.wav\"]\n",
    "# target_audios = random.sample(target_audios, len(target_audios[:10]))\n",
    "\n",
    "print(\"Speaker:\", target_speaker)\n",
    "print(\"Source Audio:\")\n",
    "display(Audio(source_audio, rate=svc_model.target_sample, autoplay=False))\n",
    "print(f\"Target Audio ({len(target_audios)})\")\n",
    "display(Audio(target_audios[0], rate=svc_model.target_sample, autoplay=False))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mel = inference(\n",
    "    source_audio,\n",
    "    target_audios,\n",
    "    target_speaker=target_speaker,\n",
    "    f0_predictor=\"rmvpe\",\n",
    "    cluster_infer_ratio=0.7,\n",
    "    n_timesteps=16,\n",
    "    temperature=1.0,\n",
    "    guidance_scale=0.5,\n",
    "    solver=\"rk4\",\n",
    ")\n",
    "\n",
    "print(f\"VC Time: {time.time() - start_time}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "mel = mel.clone()\n",
    "\n",
    "audio = generator(mel).data.cpu().float().numpy()\n",
    "\n",
    "print(f\"Decoder Time: {time.time() - start_time}\")\n",
    "\n",
    "\n",
    "soundfile.write(\n",
    "    f\"/home/alexander/Projekte/so-vits-svc/raw/{target_speaker.lower()}.wav\",\n",
    "    audio,\n",
    "    44100,\n",
    "    format=\"WAV\",\n",
    "    subtype=\"PCM_16\",\n",
    ")\n",
    "\n",
    "display(Audio(audio, rate=44100, autoplay=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy(), return_figure=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from modules.mel_processing import mel_spectrogram_torch\n",
    "import utils\n",
    "import torchaudio\n",
    "\n",
    "audio, sr = torchaudio.load(source_audio)\n",
    "mel_gen = mel_spectrogram_torch(audio, 1024, 80, 22050, 256, 1024, 0, None)\n",
    "utils.plot_spectrogram_to_numpy(mel_gen[0].cpu().numpy(), return_figure=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from glob import glob\n",
    "\n",
    "# /home/alexander/Projekte/vits-emotts/samples/Test.wav\n",
    "source_audio = \"/home/alexander/Projekte/so-vits-svc/samples/Eminem - Rap God.wav\"\n",
    "\n",
    "speakers = [\n",
    "    \"Fletcher_g1\",\n",
    "    \"Thorus_g1\",\n",
    "    \"Inquisitor\",\n",
    "    \"Vatras_g2\",\n",
    "    \"Gorn_g2\",\n",
    "    \"Diego_g1\",\n",
    "    \"Halvor_g2\",\n",
    "    \"Ulthar_g2\",\n",
    "    \"Xardas_g2\",\n",
    "    \"Milten_g1\",\n",
    "    \"Lester_g1\",\n",
    "    \"Player_g3\",\n",
    "    \"Zuben_g3\",\n",
    "    \"Grompel_g3\",\n",
    "]\n",
    "for target_speaker in speakers:\n",
    "    # target_speaker = random.choice(list(speaker_files.keys())) #\"Vatras_g2\"\n",
    "    target_audios = speaker_files[target_speaker]\n",
    "    target_audios = random.sample(target_audios, len(target_audios[:10]))\n",
    "\n",
    "    # target_audios = glob(\"/home/alexander/Projekte/so-vits-svc/samples/Suchender/*.wav\", recursive=True)\n",
    "    # target_audios = random.sample(target_audios, len(target_audios[:10]))\n",
    "\n",
    "    # target_audios = [\"/home/alexander/Projekte/so-vits-svc/samples/2.wav\", \"/home/alexander/Projekte/so-vits-svc/samples/3.wav\", \"/home/alexander/Projekte/so-vits-svc/samples/4.wav\"]\n",
    "    # target_audios = random.sample(target_audios, len(target_audios[:10]))\n",
    "\n",
    "    print(\"Speaker:\", target_speaker)\n",
    "    print(f\"Target Audio ({len(target_audios)})\")\n",
    "    display(Audio(target_audios[0], rate=svc_model.target_sample, autoplay=False))\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    mel = inference(\n",
    "        source_audio,\n",
    "        target_audios,\n",
    "        target_speaker=target_speaker,\n",
    "        f0_predictor=\"rmvpe\",\n",
    "        cluster_infer_ratio=0.7,\n",
    "        n_timesteps=15,\n",
    "        temperature=1.0,\n",
    "        guidance_scale=0.5,\n",
    "        solver=\"midpoint\",\n",
    "    )\n",
    "\n",
    "    print(f\"VC Time: {time.time() - start_time}\")\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    mel = mel.clone()\n",
    "\n",
    "    audio = generator(mel).data.squeeze(0).cpu().float().numpy()\n",
    "\n",
    "    print(f\"Decoder Time: {time.time() - start_time}\")\n",
    "\n",
    "    soundfile.write(\n",
    "        f\"/home/alexander/Projekte/so-vits-svc/raw/{target_speaker.lower()}.wav\",\n",
    "        audio,\n",
    "        44100,\n",
    "        format=\"WAV\",\n",
    "        subtype=\"PCM_16\",\n",
    "    )\n",
    "\n",
    "    # # numpy_audio = audio.squeeze(0).cpu().detach().numpy()\n",
    "    # # audio = generator(mel_spec).squeeze(0).cpu().detach().numpy()\n",
    "    # display(Audio(audio, rate=44100, autoplay=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_audio = torch.from_numpy(audio)\n",
    "mel = mel_spectrogram_torch(augmented_audio, 2048, 80, 44100, 512, 2048, 0, 11000)\n",
    "utils.plot_spectrogram_to_numpy(mel[0].cpu().numpy(), return_figure=True)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FeatureShiftModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, feature_dim=192, num_latents=32, cluster_infer_ratio=0.5, latent_dim=128\n",
    "    ):\n",
    "        super(FeatureShiftModel, self).__init__()\n",
    "        self.feature_dim = feature_dim\n",
    "        self.num_latents = num_latents\n",
    "        self.cluster_infer_ratio = cluster_infer_ratio\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder: Source features to latent space\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(feature_dim, latent_dim),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(latent_dim, latent_dim),\n",
    "        )\n",
    "\n",
    "        # Decoder: Latent space (combined with target latents) to shifted features\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim + feature_dim, feature_dim),  # Assuming concatenation\n",
    "            nn.LeakyReLU(0.2),\n",
    "            nn.Linear(feature_dim, feature_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, source_features, target_latents):\n",
    "        # Encode source features\n",
    "        # Assuming source_features is of shape (batch, feature_dim, sequence_length)\n",
    "        # Need to permute to apply linear layer: (batch, sequence_length, feature_dim)\n",
    "        source_features_permuted = source_features.permute(0, 2, 1)\n",
    "        encoded_features = self.encoder(source_features_permuted)\n",
    "\n",
    "        # Aggregate target latents\n",
    "        aggregated_target_latents = target_latents.mean(dim=2, keepdim=True)\n",
    "        expanded_target_latents = aggregated_target_latents.expand(\n",
    "            -1, -1, source_features.shape[2]\n",
    "        )\n",
    "\n",
    "        # Combine encoded source features with target latents\n",
    "        # For simplicity, we concatenate along the feature dimension\n",
    "        combined = torch.cat(\n",
    "            (encoded_features, expanded_target_latents.permute(0, 2, 1)), dim=-1\n",
    "        )\n",
    "\n",
    "        # Decode the combined representation to get shifted features\n",
    "        shifted_features = self.decoder(combined)\n",
    "\n",
    "        shifted_features = shifted_features.permute(\n",
    "            0, 2, 1\n",
    "        )  # Permute back to original shape\n",
    "\n",
    "        # Combine decoded features with original source features (optional)\n",
    "        shifted_features = (\n",
    "            self.cluster_infer_ratio * shifted_features\n",
    "            + (1 - self.cluster_infer_ratio) * source_features\n",
    "        )\n",
    "\n",
    "        return shifted_features\n",
    "\n",
    "\n",
    "# Example usage\n",
    "batch_size = 4\n",
    "sequence_length = 182\n",
    "cluster_infer_ratio = 0.5\n",
    "\n",
    "model = FeatureShiftModel(\n",
    "    feature_dim=192,\n",
    "    num_latents=32,\n",
    "    cluster_infer_ratio=cluster_infer_ratio,\n",
    "    latent_dim=128,\n",
    ")\n",
    "source_features = torch.randn(batch_size, 192, sequence_length)\n",
    "target_latents = torch.randn(batch_size, 192, 32)  # Target latents\n",
    "\n",
    "shifted_features = model(source_features, target_latents)\n",
    "print(shifted_features.shape)  # Expected shape: (batch, 192, sequence length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import modules.attentions as attentions\n",
    "from modules.feature_encoder_decoder import FeatureEncoderDecoder\n",
    "\n",
    "# Example usage\n",
    "batch_size = 4\n",
    "sequence_length = 182\n",
    "\n",
    "model = FeatureEncoderDecoder(hidden_channels=192)\n",
    "x = torch.randn(batch_size, 192, sequence_length)  # Adjusted shape for LSTM\n",
    "x_mask = torch.ones(batch_size, 1, sequence_length)\n",
    "prosody_latent = torch.randn(batch_size, 192, 32)  # Target latents\n",
    "prosody_mask = torch.ones(batch_size, 1, 32)\n",
    "\n",
    "shifted_features = model(\n",
    "    x, x_mask, prosody_latent, prosody_mask\n",
    ")  # Transpose back to original shape\n",
    "print(shifted_features.shape)  # Expected shape: (batch, sequence_length, 192)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
